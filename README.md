# ğŸ§  RAG-Based Telegram Question Answering Bot (Persian)

This is a Retrieval-Augmented Generation (RAG) based Q&A system with Telegram integration. It answers Persian-language questions based on local document context using Google's **Gemini API** (via OpenAI-compatible endpoint), **FAISS vector search**, and a **Telegram bot interface**.

---

## ğŸ“¦ Features

- âœ… Persian question answering from `.txt` documents in `/data/`
- âœ… FAISS vector search for efficient document retrieval
- âœ… Gemini API (via `openai-python`) for LLM responses
- âœ… FastAPI backend exposed via `/ask`
- âœ… Real-time Telegram bot integration
- âœ… Dockerized with Docker Compose
- âœ… Tests for LLM, API, and bot interaction

---

## ğŸ§° Technologies Used

- FastAPI
- FAISS
- HuggingFace Transformers / SentenceTransformers
- Gemini API via OpenAI-compatible client
- python-telegram-bot
- Docker & Docker Compose
- Pytest

---

## ğŸš€ Quickstart (Local + Docker)

### 1. ğŸ“ Clone the Repository

```bash
git clone https://github.com/AlirezaFT78/Mohaymen_AI_Chatbot.git
cd Mohaymen_AI_Chatbot
```

---

### 2. âš™ï¸ Setup `.env` file

Create a `.env` file in the root directory:

```env
OPENAI_API_KEY="your_api_key_here"
OPENAI_API_BASE="https://your_openai_base_url_here"
TELEGRAM_BOT_TOKEN="your_telegram_bot_token_here"
FASTAPI_URL="http://your_fastapi_url_here"
TEST_CHAT_ID="your_test_chat_id_here"
LLM_MODEL="your_chosen_model_here"
```

> In Docker, `FASTAPI_URL` should be set to: `http://fastapi:8000/ask`

---

### 3. ğŸ³ Run with Docker Compose

```bash
docker compose up --build
```

- FastAPI: [http://localhost:8000/docs](http://localhost:8000/docs)
- Telegram bot: responds to user messages

---

## ğŸ§ª Example Persian Q&A

**User Question:**

```
Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø¶Ø±ÛŒØ¨ Ø­Ù‚ÙˆÙ‚ Ø¨Ø±Ø§ÛŒ Ú©Ø¯Ø§Ù… Ú¯Ø±ÙˆÙ‡ Ø´ØºÙ„ÛŒ Ø§Ø³ØªØŸ
```

**Bot Answer:**

```
â€«Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ØŒ Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø¶Ø±ÛŒØ¨ Ø­Ù‚ÙˆÙ‚ Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙˆÙ‡ Ø´ØºÙ„ÛŒ "ØªÙˆØ³Ø¹Ù‡ Ù†Ø±Ù… Ø§ÙØ²Ø§Ø±-Back End" Ùˆ "Ø§Ù…Ù†ÛŒØª" Ø¨Ø§ Ø¶Ø±ÛŒØ¨ 3.1 Ø§Ø³Øª.
```

---

## ğŸ“ Project Structure

```
rag-telegram-bot/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ telegram_bot.py      # Telegram bot logic
â”‚   â”œâ”€â”€ llm.py               # Gemini/OpenAI API handler
â”‚   â”œâ”€â”€ rag.py               # RAG orchestration logic
â”‚   â”œâ”€â”€ vector_store.py      # FAISS storage and retrieval
â”‚
â”œâ”€â”€ data/                    # Input .txt documents
â”œâ”€â”€ hf_model_cache/          # Model cache directory
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_1_llm.py        # LLM API unit test
â”‚   â”œâ”€â”€ test_2_api.py        # FastAPI /ask test
â”‚   â”œâ”€â”€ test_3_bot.py        # Bot behavior unit test
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ start.sh
â”œâ”€â”€ README.md
â””â”€â”€ answers.md
```

---

## ğŸ§ª Running Tests

Make sure the virtualenv is active or you're in the container:

```bash
pytest tests/
```

---

## ğŸ“Œ Future Improvements

- Add persistent FAISS volume
- Use webhooks instead of polling for bot
- Add query caching to minimize Gemini usage
- Add Persian-specific tokenizer or chunking

---

## ğŸ“ƒ License

MIT License
