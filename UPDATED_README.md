
# ğŸ§  RAG-Based Telegram Question Answering Bot (Persian)

This is a Retrieval-Augmented Generation (RAG) based Q&A system with Telegram integration. It answers Persian-language questions based on local document context using Google's **Gemini API** (via OpenAI-compatible endpoint), **FAISS vector search**, and a **Telegram bot interface**.

---

## ğŸ“¦ Features

- âœ… Persian question answering from `.txt` documents in `/data/`
- âœ… FAISS vector search with overlapping chunks and token-aware truncation
- âœ… Gemini API (via `openai-python`) for LLM responses
- âœ… FastAPI backend exposed via `/ask` endpoint with health checks
- âœ… Real-time Telegram bot integration with anti-flooding
- âœ… Dockerized with Docker Compose and volume persistence
- âœ… Query caching with `lru_cache`
- âœ… Tests for LLM, API, and bot interaction with mocking support
- âœ… Supports Hugging Face model caching and `.env` config

---

## ğŸ§° Technologies Used

- FastAPI
- FAISS
- HuggingFace Transformers / SentenceTransformers
- Gemini API via OpenAI-compatible client
- python-telegram-bot
- Docker & Docker Compose
- Pytest

---

## ğŸš€ Quickstart (Local + Docker)

### 1. ğŸ“ Clone the Repository

```bash
git clone https://github.com/AlirezaFT78/Mohaymen_AI_Chatbot.git
cd Mohaymen_AI_Chatbot
```

---

### 2. âš™ï¸ Setup `.env` file

Create a `.env` file in the root directory:

```env
OPENAI_API_KEY="your_api_key_here"
OPENAI_API_BASE="https://your_openai_base_url_here"
TELEGRAM_BOT_TOKEN="your_telegram_bot_token_here"
FASTAPI_URL="http://fastapi:8000/ask"
TEST_CHAT_ID="your_test_chat_id_here"
OPENAI_MODEL="gemini-2.0-flash-lite"
```

---

### 3. ğŸ³ Run with Docker Compose

```bash
docker compose up --build
```

- FastAPI: [http://localhost:8000/docs](http://localhost:8000/docs)
- Telegram bot: responds to user messages

---

## ğŸ¤– Telegram Bot Demo

You can interact with the deployed chatbot via Telegram here:

ğŸ‘‰ [@Mohaymen_AI_Task_bot](https://t.me/Mohaymen_AI_Task_bot)

Simply send a question in Persian (based on the loaded documents), and the bot will respond with a context-aware answer powered by Gemini and FAISS.

---

## ğŸ§ª Example Persian Q&A

**User Question:**

```
Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø¶Ø±ÛŒØ¨ Ø­Ù‚ÙˆÙ‚ Ø¨Ø±Ø§ÛŒ Ú©Ø¯Ø§Ù… Ú¯Ø±ÙˆÙ‡ Ø´ØºÙ„ÛŒ Ø§Ø³ØªØŸ
```

**Bot Answer:**

```
â€«Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ØŒ Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø¶Ø±ÛŒØ¨ Ø­Ù‚ÙˆÙ‚ Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙˆÙ‡ Ø´ØºÙ„ÛŒ "ØªÙˆØ³Ø¹Ù‡ Ù†Ø±Ù… Ø§ÙØ²Ø§Ø±-Back End" Ùˆ "Ø§Ù…Ù†ÛŒØª" Ø¨Ø§ Ø¶Ø±ÛŒØ¨ 3.1 Ø§Ø³Øª.
```

---

## ğŸ“ Project Structure

```
rag-telegram-bot/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ telegram_bot.py      # Telegram bot logic
â”‚   â”œâ”€â”€ llm.py               # Gemini/OpenAI API handler
â”‚   â”œâ”€â”€ rag.py               # RAG orchestration logic
â”‚   â”œâ”€â”€ vector_store.py      # FAISS storage and retrieval
â”‚
â”œâ”€â”€ data/                    # Input .txt documents
â”œâ”€â”€ hf_model_cache/          # Model cache directory
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_1_llm.py        # LLM API unit test
â”‚   â”œâ”€â”€ test_2_api.py        # FastAPI /ask test
â”‚   â”œâ”€â”€ test_3_bot.py        # Bot behavior unit test
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ start.sh
â”œâ”€â”€ README.md
â””â”€â”€ answers.md
```

---

## ğŸ§ª Running Tests

Make sure the virtualenv is active or you're in the container:

```bash
pytest tests/
```

---

## ğŸ“Œ Future Improvements

- Use Redis or persistent caching instead of in-memory
- Use webhooks instead of polling for Telegram bot
- Add document metadata in responses
- Implement admin panel for document upload
- Optimize Persian chunking with language-aware splitters

---

## ğŸ“ƒ License

MIT License
